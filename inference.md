# ONNX
onnx是把torch模型用protobuf序列化后的产物，通过跨平台和语言的onnx runtime api进行推理调用。
优势:
* 训练的时候torch维护的是一个动态计算图，为了反向传播，会在layer间缓存很多状态，而推理不需要，因为没有反向计算。
* 因为推理没有反向计算，所以不用考虑梯度怎么传播，一些算子可以融合计算。
* 静态计算图的优势，可以提前分配好显存

# TensorRT
英伟达提供的在N卡上推理的runtime api。
相比ORT进一步优化:
* 算法上，TensorRT维护了几十种矩阵乘法算法，用于特定处理不同shape的矩阵输入。甚至每种算子可能也有几十种实现，针对不同的N卡定向优化。
* 参数量化，用更低精度的数值类型代替高精度数值类型参与计算。
* 其他手法，比如更激进的算子融合
TensorRT用的时候会有一个compile过程，这里其实是根据模型参数和具体硬件类型来生成量化的参数，以及优化计算图等等，结果称为`enginee`。
所以部署的时候可以做一次warmup，把enginee缓存起来。

TensorRT有很多不支持的cuda算子，所以工程上一般会选择ORT + TensorRT的方式部署模型，推理前框架会对计算图做划分，看哪些算子让TensorRT计算，
哪些给ORT计算。
